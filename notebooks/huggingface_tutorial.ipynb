{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LalitaDeelert/NLP-ZH_TH-Project/blob/main/notebooks/huggingface_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Pretraining Seq2Seq with MarianMT and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJHLVMb2Tkj2",
    "outputId": "d15a05fb-66d0-4149-dfab-6336a3ccbf31"
   },
   "outputs": [],
   "source": [
    "# !pip -q install datasets transformers==4.6.0 sentencepiece pythainlp\n",
    "#install sacrebleu with thai\n",
    "# !pip uninstall -q sacrebleu\n",
    "# !pip install -q git+https://github.com/cstorm125/sacrebleu.git@add_thai_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "up1ut6SZTkj3",
    "outputId": "49a78c7d-6c69-424b-954c-845f3a4d0aa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import sentencepiece as spm\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook details how to pretrain a seq2seq model with `MarianMT` model using HuggingFace. In this example, we will train a `zh_cn` to `th` machine translation model using `open_subtitles` data. We refer to [`Helsinki-NLP/opus-mt-en-ro`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) checkpoint as basis for our pretraining. The notebook is adapted from [Transformers Notebooks](https://github.com/huggingface/notebooks/blob/master/examples/translation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ü§ó Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`. We use the `zh_cn`/`th` part of the `open_subtitles` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rpu8-xkEXG4I",
    "outputId": "e6998783-4dc9-4b81-cca0-6162ab8896f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237445</th>\n",
       "      <td>ÊàëÂ∏åÊúõ‰ªñ‰ª¨ËÉΩÂøçÂèóÊØîÈÇ£Êõ¥Â∞ëÁöÑÊ∏∏Êàè‰ª∑ÂÄºÂπ∂‰∏î‰∏çË¶ÅÈÇ£‰πàË¥µ„ÄÇ</td>\n",
       "      <td>Ôøº‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏°‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237446</th>\n",
       "      <td>Â∏åÊúõ‰ªñ‰ª¨ËÉΩ‰øÆÂ§çÂÆÉÔºåÂõ†‰∏∫ÂõæÁâáÂæàÊ£í„ÄÇ</td>\n",
       "      <td>Ôøº‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏°‡∏±‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ô‡∏±‡πâ‡∏ô‡∏™‡πç‡∏≤‡∏Ñ‡∏±‡∏ç‡∏Ñ‡πà‡∏∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237447</th>\n",
       "      <td>ÂàöÊääÊàëÁöÑÂÖÖÁîµÂô®‰ªéÂÖÖÁîµÂô®‰∏äÂèñ‰∏ãÊù•ÔºåÂÆÉ‰ºº‰πéÂÖÖÁîµÊ≠£Â∏∏ÔºåÁÑ∂ÂêéÂΩìÊàë‰ªäÂ§©Êó©‰∏äÂùêÂú®ÁîµËÑëÂâçÊó∂Á™ÅÁÑ∂ÊîæÁîµ„ÄÇ</td>\n",
       "      <td>Ôøº‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ñ‡∏≠‡∏î‡∏™‡∏≤‡∏¢‡∏ä‡∏≤‡∏£‡πå‡∏à‡∏≠‡∏≠‡∏Å‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏ô‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ä‡∏≤‡∏£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237448</th>\n",
       "      <td>Á§º‰ª™ÈÄ†Â∞±‰∫∫„ÄÇ</td>\n",
       "      <td>ÔøΩÔøΩÔøΩ‡∏≤‡∏£‡∏¢‡∏≤‡∏ó‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237449</th>\n",
       "      <td>üò¢ ÂîØ‰∏ÄÁº∫Â∞ëÁöÑÂ∞±ÊòØËøô‰∏™ÔºåÂ±èÂπïÂ∞ÅÈù¢‰πüÊúâ:) ÂñúÊ¨¢ÂÆÉÁöÑÂ§ñËßÇÔºÅÊÄªÁöÑÊù•ËØ¥ÔºåÂØπÂÆÉÈùûÂ∏∏Êª°ÊÑèÔºÅ</td>\n",
       "      <td>üò¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏õ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏¥‡∏î‡∏à‡∏≠‡∏Ñ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  zh  \\\n",
       "1237445                    ÊàëÂ∏åÊúõ‰ªñ‰ª¨ËÉΩÂøçÂèóÊØîÈÇ£Êõ¥Â∞ëÁöÑÊ∏∏Êàè‰ª∑ÂÄºÂπ∂‰∏î‰∏çË¶ÅÈÇ£‰πàË¥µ„ÄÇ   \n",
       "1237446                             Â∏åÊúõ‰ªñ‰ª¨ËÉΩ‰øÆÂ§çÂÆÉÔºåÂõ†‰∏∫ÂõæÁâáÂæàÊ£í„ÄÇ   \n",
       "1237447  ÂàöÊääÊàëÁöÑÂÖÖÁîµÂô®‰ªéÂÖÖÁîµÂô®‰∏äÂèñ‰∏ãÊù•ÔºåÂÆÉ‰ºº‰πéÂÖÖÁîµÊ≠£Â∏∏ÔºåÁÑ∂ÂêéÂΩìÊàë‰ªäÂ§©Êó©‰∏äÂùêÂú®ÁîµËÑëÂâçÊó∂Á™ÅÁÑ∂ÊîæÁîµ„ÄÇ   \n",
       "1237448                                       Á§º‰ª™ÈÄ†Â∞±‰∫∫„ÄÇ   \n",
       "1237449     üò¢ ÂîØ‰∏ÄÁº∫Â∞ëÁöÑÂ∞±ÊòØËøô‰∏™ÔºåÂ±èÂπïÂ∞ÅÈù¢‰πüÊúâ:) ÂñúÊ¨¢ÂÆÉÁöÑÂ§ñËßÇÔºÅÊÄªÁöÑÊù•ËØ¥ÔºåÂØπÂÆÉÈùûÂ∏∏Êª°ÊÑèÔºÅ   \n",
       "\n",
       "                                                        th  \n",
       "1237445  Ôøº‡∏ä‡∏±‡πâ‡∏ô‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏°‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß...  \n",
       "1237446   Ôøº‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏°‡∏±‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ô‡∏±‡πâ‡∏ô‡∏™‡πç‡∏≤‡∏Ñ‡∏±‡∏ç‡∏Ñ‡πà‡∏∞  \n",
       "1237447  Ôøº‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ñ‡∏≠‡∏î‡∏™‡∏≤‡∏¢‡∏ä‡∏≤‡∏£‡πå‡∏à‡∏≠‡∏≠‡∏Å‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏ô‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ä‡∏≤‡∏£...  \n",
       "1237448                                    ÔøΩÔøΩÔøΩ‡∏≤‡∏£‡∏¢‡∏≤‡∏ó‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ô  \n",
       "1237449  üò¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏õ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏¥‡∏î‡∏à‡∏≠‡∏Ñ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data from csv\n",
    "df = pd.read_csv('../data/v1/Train.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RUadZwO-Xpvr"
   },
   "outputs": [],
   "source": [
    "#convert to dictionary\n",
    "j = {'translation':[]}\n",
    "for i in df.itertuples():\n",
    "    j['translation'] += [{'zh_cn':i[1], 'th':i[2]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwxqC2q6ZIoz",
    "outputId": "2424f08a-6fa5-4c73-961b-6db0e0f9b960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1225075\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 12375\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load into dataset\n",
    "train_dataset = Dataset.from_dict(j)\n",
    "raw_datasets = train_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "fe341308-f0d7-4963-f915-085f7816c6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'th': '‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á‡∏¢‡∏µ‡∏ô‡∏™‡πå‡∏â‡∏±‡∏ô‡∏Ñ‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ô‡∏µ‡πà‡∏¢', 'zh_cn': 'ÊúÄËøëËÉñÂà∞Áâõ‰ªîË£§ÈÉΩÁ©ø‰∏ç‰∏ã'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks]['translation'])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "SZy5tRB_IrI7",
    "outputId": "046010b6-4106-42f8-a5fa-547f98931b1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>th</th>\n",
       "      <th>zh_cn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∏‡πà‡∏°‡∏•‡∏∂‡∏Å‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞‡πÅ‡∏ï‡πà‡∏°‡∏±‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÜ‡∏Å‡πá‡∏à‡∏ö</td>\n",
       "      <td>Âπ∂‰∏çÊòØËØ¥ÂÆÉÊ≤°ÊúâÊÉÖËäÇÊàñÊ∑±Â∫¶ÔºåÊïÖ‰∫ãÂè™ÊòØÂÅúÊ≠¢‰∫Ü„ÄÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∏à‡∏µ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏≤‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏∏‡∏î‡∏°‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡πç‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ß‡∏≤‡∏î‡∏Å‡∏•‡∏±‡∏ß ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏ü‡∏¥‡∏•‡∏¥‡∏õ‡∏õ‡∏¥‡∏ô‡∏™‡πå‡πÅ‡∏•‡∏∞‡∏î‡πç‡∏≤‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà ‡∏û‡∏•.‡∏≠.‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏≤‡∏ô‡∏¥ ‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡πÇ‡∏´‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ ‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏¥‡∏ô‡∏•‡∏≤‡∏î‡∏ï‡∏£‡∏∞‡πÄ‡∏ß‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡∏ô‡∏±‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢‡∏ñ‡∏∂‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á</td>\n",
       "      <td>‰∏≠ÂõΩ‰∏ÄÁõ¥Âú®ËµÑÊ∫ê‰∏∞ÂØåÁöÑÂπøË¢§Âú∞Âå∫Âª∫ÈÄ†‰∫∫Â∑•Â≤õÔºå‰ª§Âë®ËæπÂõΩÂÆ∂ÊÑüÂà∞ÈúáÊÉä„ÄÇÊó•Êú¨Âä†Âº∫‰∫Ü‰∏éËè≤ÂæãÂÆæÁöÑÈò≤Âä°Âêà‰ΩúÔºåÂπ∂Âú®ËØ•Âú∞Âå∫ËøõË°å‰∫ÜËÅîÂêàÊºî‰π†„ÄÇÊó•Êú¨Èò≤Âç´Â§ßËá£‰∏≠Ë∞∑Â∞ÜÂÜõË°®Á§∫ÔºåÂ¶ÇÊûúÁ´ãÊ≥ïËé∑ÂæóÊâπÂáÜÔºåÊó•Êú¨ÂèØ‰ª•ÂêëËØ•Âú∞Âå∫Ê¥æÈÅ£‰æ¶ÂØüÊú∫Ôºå‰ΩÜ‰ªñÂê¶ËÆ§‰∫Ü‰ªª‰ΩïÂÖ∑‰ΩìËÆ°Âàí„ÄÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢</td>\n",
       "      <td>ÊàëÊÉ≥Â∏Æ‰Ω†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡∏Ç‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏à‡∏∞‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡πà‡∏≤‡∏ô ‡πÑ‡∏Å‡∏≠‡∏±‡∏™</td>\n",
       "      <td>ÊàëË¶ÅÊÑüË∞¢‰Ω†ÔºåÁõñ‰πåÊñØ„ÄÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡∏´‡∏ô‡∏π‡πÅ‡∏Ñ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° ‡∏´‡∏ô‡∏π‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏´‡∏ô‡∏π‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£</td>\n",
       "      <td>ÊàëÂè™ÊòØËøòÊ≤°ÂáÜÂ§áÂ•Ω„ÄÇÊàë‰∏çÁü•ÈÅìÊàëÊòØË∞Å„ÄÇ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5o4rUteaIrI_",
    "outputId": "e1733bff-ffa0-443a-d2a9-6a9f054cf611"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: The system stream (a sequence of segments)\n",
       "    references: A list of one or more reference streams (each a sequence of segments)\n",
       "    smooth: The smoothing method to use\n",
       "    smooth_value: For 'floor' smoothing, the floor to use\n",
       "    force: Ignore data that looks already tokenized\n",
       "    lowercase: Lowercase the data\n",
       "    tokenize: The tokenizer to use\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "    >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "    >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "    >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "    >>> print(round(results[\"score\"], 1))\n",
       "    100.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings (list of list for the labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN1Rq0aIrJC",
    "outputId": "7757566c-da6a-4b32-e120-481ea0be171b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 100.00000000000004,\n",
       " 'counts': [4, 3, 2, 1],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [100.0, 100.0, 100.0, 100.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there general kenobi\"]\n",
    "fake_labels = [[\"hello there general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iUsPbS4TkkG"
   },
   "source": [
    "## Train Tokenizer with `sentencepiece`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4C8vRYnTkkH"
   },
   "source": [
    "`MarianMT` requires 2 `sentencepiece` tokenizers for `source.spm` and `target.spm` languages. We will use the same tokenizer trained on both languages for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8e49MrFTkkH"
   },
   "source": [
    "Save training set to text files for tokenizer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5RAyYaV4TkkI"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(raw_datasets['train']['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SipP3BGdTkkI"
   },
   "outputs": [],
   "source": [
    "# #zh_cn\n",
    "# train_df['zh_cn'].to_csv('zh_cn_tokenizer_train.txt',header=None,index=None)\n",
    "\n",
    "# #th\n",
    "# train_df['th'].to_csv('th_tokenizer_train.txt',header=None,index=None)\n",
    "\n",
    "#both languages\n",
    "train_df.to_csv('zh_cn_th_tokenizer_train.txt',header=None, index=None, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YdtX_8elTkkI"
   },
   "outputs": [],
   "source": [
    "vocab_size = 32000 #choose what you want\n",
    "user_defined_symbols = '<pad>'\n",
    "vocab_special_size = vocab_size + len(user_defined_symbols)\n",
    "\n",
    "#train tokenizer\n",
    "def train_spm_tokenizer(train_fname, \n",
    "                        model_prefix,\n",
    "                        vocab_special_size,\n",
    "                        model_dir,\n",
    "                        character_coverage=0.9995,\n",
    "                        max_sentencepiece_length=16,\n",
    "                        add_dummy_prefix='false',\n",
    "                        model_type='unigram',\n",
    "                        user_defined_symbols='<pad>'):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    spm.SentencePieceTrainer.train((f'--input={train_fname} '\n",
    "                                   f'--model_prefix={model_prefix} '\n",
    "                                   f'--vocab_size={vocab_special_size} '\n",
    "                                   f'--character_coverage={character_coverage} '\n",
    "                                   f'--max_sentencepiece_length={max_sentencepiece_length} '\n",
    "                                   f'--add_dummy_prefix={add_dummy_prefix} '\n",
    "                                   f'--model_type={model_type} '\n",
    "                                   f'--user_defined_symbols={user_defined_symbols}'))\n",
    "    !mkdir $model_dir; mv $model_prefix* $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "J3R5VY-sTkkJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòmarian-mt-zh_cn-th‚Äô: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# #train zh_cn spm\n",
    "# train_spm_tokenizer(train_fname='zh_cn_tokenizer_train.txt',\n",
    "#                     model_prefix='source',\n",
    "#                     vocab_special_size=vocab_special_size, #+1 for <pad>\n",
    "#                     model_dir='marian-mt-zh_cn-th'\n",
    "#                    ) \n",
    "\n",
    "# #train th spm\n",
    "# train_spm_tokenizer(train_fname='th_tokenizer_train.txt',\n",
    "#                     model_prefix='target',\n",
    "#                     vocab_special_size=vocab_special_size, #+1 for <pad>\n",
    "#                     model_dir='marian-mt-zh_cn-th'\n",
    "#                    ) \n",
    "\n",
    "#train both spm\n",
    "train_spm_tokenizer(train_fname='zh_cn_th_tokenizer_train.txt',\n",
    "                    model_prefix='both',\n",
    "                    vocab_special_size=vocab_special_size,\n",
    "                    model_dir='marian-mt-zh_cn-th'\n",
    "                   ) \n",
    "\n",
    "#copy both.model to source.spm and target.spm for marianmt tokenizer\n",
    "!cp marian-mt-zh_cn-th/both.model marian-mt-zh_cn-th/source.spm\n",
    "!cp marian-mt-zh_cn-th/both.model marian-mt-zh_cn-th/target.spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv-hYpBrTkkJ",
    "outputId": "f8ebafd8-9286-41cd-a475-221d93844c37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('marian-mt-zh_cn-th/both.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF5ASFuBTkkK",
    "outputId": "0032fbb1-4743-4256-9e56-4072e280269c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Âíå', 'Âà´‰∫∫', '‰∏ÄËµ∑', 'Êù•', 'Âêó', '?', '‚ñÅ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode_as_pieces('ÂíåÂà´‰∫∫‰∏ÄËµ∑Êù•Âêó? ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.225075e+06\n",
       "mean     1.389428e+01\n",
       "std      1.886827e+01\n",
       "min      1.000000e+00\n",
       "25%      6.000000e+00\n",
       "50%      9.000000e+00\n",
       "75%      1.500000e+01\n",
       "max      3.620000e+02\n",
       "Name: th, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.th.map(lambda x: len(sp.encode_as_pieces(x))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.th.map(lambda x: len(sp.encode_as_pieces(x))).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.225075e+06\n",
       "mean     1.422530e+01\n",
       "std      1.793655e+01\n",
       "min      1.000000e+00\n",
       "25%      7.000000e+00\n",
       "50%      1.000000e+01\n",
       "75%      1.600000e+01\n",
       "max      3.290000e+02\n",
       "Name: zh_cn, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.zh_cn.map(lambda x: len(sp.encode_as_pieces(x))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAASQklEQVR4nO3cf4wcZ33H8fe3dkJpLkpKDKfIDnVAhsjNDxpfE9RSuIMCdvjDrZS2CVaAKKkVCdNWalpcIQEVfxSKUlGkBMsFK6QqnNqSgkssUoSyDWlIMW7zw07q4MYWGCOsEDBdIzXYfPvHjmF97N7unte7O0/fL2l1OzPP3H5uTv547tnZicxEklR/PzfuAJKk4bDQJakQFrokFcJCl6RCWOiSVAgLXZIKMdZCj4gdEXE0Ivb2Of53I+LJiNgXEZ862/kkqU5inNehR8RrgSZwT2Ze3mPsGuDvgddn5vci4iWZeXQUOSWpDsZ6hp6ZDwLPta+LiJdHxBciYk9EfDkiLqs2/T5wZ2Z+r9rXMpekNpM4h74deFdmrgNuB+6q1r8CeEVE/FtEPBIR68eWUJIm0PJxB2gXEVPArwH/EBGnVr+g+rocWAPMAquAL0fE5Zn5/RHHlKSJNFGFTusvhu9n5qs6bDsMPJKZPwIORsR+WgW/e4T5JGliTdSUS2b+gFZZ/w5AtFxVbf4sMFetX0FrCuaZceSUpEk07ssWPw18BXhlRByOiFuATcAtEfEYsA/YWA2/H/huRDwJPAD8SWZ+dxy5JWkSjfWyRUnS8EzUlIskaenG9qboihUrcvXq1QPvd/z4cc4777zhBxqBumava26ob3Zzj15dsu/Zs+fZzHxxx42ZuegD2AEcBfZ22b4JeLx6PAxc1et7Zibr1q3LpXjggQeWtN8kqGv2uubOrG92c49eXbIDX8suvdrPlMvdwGIf4jkIvC4zrwQ+QOuDQZKkEes55ZKZD0bE6kW2P9y2+AitD/1Ikkasr6tcqkL/fPa+gdbtwGWZeWuX7ZuBzQDT09Pr5ufnBw7cbDaZmpoaeL9JUNfsdc0N9c1u7tGrS/a5ubk9mTnTcWO3uZg8fZ58NV3m0NvGzAFPARf18z2dQ6+PuubOrG92c49eXbKzyBz6UK5yiYgrgY8DG9IP+0jSWJzxdegR8VLgXuCmzHz6zCNJkpai5xl69fH8WWBFRBwG3gecA5CZ24D3AhcBd1V3SDyR3eZ3JElnTT9XudzYY/utQMc3QSVJo+NH/yWpEJN2P/S+rN5639he+9AH3zK215akxXiGLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKkTPQo+IHRFxNCL2dtkeEfHRiDgQEY9HxNXDjylJ6qWfM/S7gfWLbN8ArKkem4GPnXksSdKgehZ6Zj4IPLfIkI3APdnyCHBhRFw8rICSpP4MYw59JfDNtuXD1TpJ0ghFZvYeFLEa+HxmXt5h233AX2TmQ9Xyl4A/zcw9HcZupjUtw/T09Lr5+fmBAzebTQ4eOznwfsNyxcoLlrxvs9lkampqiGlGo665ob7ZzT16dck+Nze3JzNnOm1bPoTvfxi4pG15FXCk08DM3A5sB5iZmcnZ2dmBX6zRaHDHQ8cHTzkkhzbNLnnfRqPBUn7mcatrbqhvdnOPXp2znzKMKZedwNuqq11eDRzLzG8P4ftKkgbQ8ww9Ij4NzAIrIuIw8D7gHIDM3AbsAq4DDgA/BG4+W2ElSd31LPTMvLHH9gTeObREkqQl8ZOiklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1Ih+ir0iFgfEfsj4kBEbO2w/YKI+OeIeCwi9kXEzcOPKklaTM9Cj4hlwJ3ABmAtcGNErF0w7J3Ak5l5FTAL3BER5w45qyRpEf2coV8DHMjMZzLzeWAe2LhgTALnR0QAU8BzwImhJpUkLSoyc/EBEdcD6zPz1mr5JuDazNzSNuZ8YCdwGXA+8HuZeV+H77UZ2AwwPT29bn5+fuDAzWaTg8dODrzfsFyx8oIl79tsNpmamhpimtGoa26ob3Zzj15dss/Nze3JzJlO25b3sX90WLfwf4E3A48CrwdeDnwxIr6cmT84bafM7cB2gJmZmZydne3j5U/XaDS446HjA+83LIc2zS5530ajwVJ+5nGra26ob3Zzj16ds5/Sz5TLYeCStuVVwJEFY24G7s2WA8BBWmfrkqQR6afQdwNrIuLS6o3OG2hNr7T7BvAGgIiYBl4JPDPMoJKkxfWccsnMExGxBbgfWAbsyMx9EXFbtX0b8AHg7oh4gtYUzbsz89mzmFuStEA/c+hk5i5g14J129qeHwHeNNxokqRB+ElRSSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQfRV6RKyPiP0RcSAitnYZMxsRj0bEvoj41+HGlCT1srzXgIhYBtwJvBE4DOyOiJ2Z+WTbmAuBu4D1mfmNiHjJWcorSeqinzP0a4ADmflMZj4PzAMbF4x5K3BvZn4DIDOPDjemJKmXyMzFB0RcT+vM+9Zq+Sbg2szc0jbmI8A5wC8D5wN/nZn3dPhem4HNANPT0+vm5+cHDtxsNjl47OTA+w3LFSsvWPK+zWaTqampIaYZjbrmhvpmN/fo1SX73Nzcnsyc6bSt55QLEB3WLfxfYDmwDngD8ELgKxHxSGY+fdpOmduB7QAzMzM5Ozvbx8ufrtFocMdDxwfeb1gObZpd8r6NRoOl/MzjVtfcUN/s5h69Omc/pZ9CPwxc0ra8CjjSYcyzmXkcOB4RDwJXAU8jSRqJfubQdwNrIuLSiDgXuAHYuWDM54DfiIjlEfELwLXAU8ONKklaTM8z9Mw8ERFbgPuBZcCOzNwXEbdV27dl5lMR8QXgceDHwMczc+/ZDC5JOl0/Uy5k5i5g14J12xYsfxj48PCiSZIG4SdFJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFaKvQo+I9RGxPyIORMTWRcb9akScjIjrhxdRktSPnoUeEcuAO4ENwFrgxohY22Xch4D7hx1SktRbP2fo1wAHMvOZzHwemAc2dhj3LuAzwNEh5pMk9Skyc/EBremT9Zl5a7V8E3BtZm5pG7MS+BTweuATwOcz8x87fK/NwGaA6enpdfPz8wMHbjabHDx2cuD9huWKlRcsed9ms8nU1NQQ04xGXXNDfbObe/Tqkn1ubm5PZs502ra8j/2jw7qF/wt8BHh3Zp6M6DS82ilzO7AdYGZmJmdnZ/t4+dM1Gg3ueOj4wPsNy6FNs0vet9FosJSfedzqmhvqm93co1fn7Kf0U+iHgUvallcBRxaMmQHmqzJfAVwXEScy87PDCClJ6q2fQt8NrImIS4FvATcAb20fkJmXnnoeEXfTmnL57PBiSpJ66VnomXkiIrbQunplGbAjM/dFxG3V9m1nOaMkqQ/9nKGTmbuAXQvWdSzyzHzHmceSJA3KT4pKUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYXoq9AjYn1E7I+IAxGxtcP2TRHxePV4OCKuGn5USdJiehZ6RCwD7gQ2AGuBGyNi7YJhB4HXZeaVwAeA7cMOKklaXD9n6NcABzLzmcx8HpgHNrYPyMyHM/N71eIjwKrhxpQk9RKZufiAiOuB9Zl5a7V8E3BtZm7pMv524LJT4xds2wxsBpienl43Pz8/cOBms8nBYycH3m9Yrlh5wZL3bTabTE1NDTHNaNQ1N9Q3u7lHry7Z5+bm9mTmTKdty/vYPzqs6/i/QETMAbcAr+m0PTO3U03HzMzM5OzsbB8vf7pGo8EdDx0feL9hObRpdsn7NhoNlvIzj1tdc0N9s5t79Oqc/ZR+Cv0wcEnb8irgyMJBEXEl8HFgQ2Z+dzjxJEn96mcOfTewJiIujYhzgRuAne0DIuKlwL3ATZn59PBjSpJ66XmGnpknImILcD+wDNiRmfsi4rZq+zbgvcBFwF0RAXCi2xyPJOns6GfKhczcBexasG5b2/NbgZ95E1SSNDp+UlSSCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBVi+bgD1M3qrfcted8/vuIE71ji/oc++JYlv66k/x88Q5ekQljoklQIC12SCmGhS1Ih+ir0iFgfEfsj4kBEbO2wPSLio9X2xyPi6uFHlSQtpudVLhGxDLgTeCNwGNgdETsz88m2YRuANdXjWuBj1VcNyZlcXXOm7l5/3theW1L/+rls8RrgQGY+AxAR88BGoL3QNwL3ZGYCj0TEhRFxcWZ+e+iJNXJPfOvYki+3HLczuVR0HLw8VWein0JfCXyzbfkwP3v23WnMSuC0Qo+IzcDmarEZEfsHStuyAnh2CfuN3R/UNHtdc0P9sseHfvK0Vrnb1DU31Cf7L3Xb0E+hR4d1uYQxZOZ2YHsfr9k9TMTXMnPmTL7HuNQ1e11zQ32zm3v06pz9lH7eFD0MXNK2vAo4soQxkqSzqJ9C3w2siYhLI+Jc4AZg54IxO4G3VVe7vBo45vy5JI1WzymXzDwREVuA+4FlwI7M3BcRt1XbtwG7gOuAA8APgZvPXuQzm7IZs7pmr2tuqG92c49enbMDEK0LUyRJdecnRSWpEBa6JBWiVoXe6xYEkyQiDkXEExHxaER8rVr3ooj4YkR8vfr6i+POCRAROyLiaETsbVvXNWtE/Fn1O9gfEW8eT+quud8fEd+qjvujEXFd27ZJyX1JRDwQEU9FxL6I+MNqfR2OebfsE33cI+LnI+KrEfFYlfvPq/UTf8wHkpm1eNB6Q/a/gZcB5wKPAWvHnWuRvIeAFQvW/SWwtXq+FfjQuHNWWV4LXA3s7ZUVWFsd+xcAl1a/k2UTlPv9wO0dxk5S7ouBq6vn5wNPV/nqcMy7ZZ/o407rszJT1fNzgH8HXl2HYz7Io05n6D+5BUFmPg+cugVBnWwEPlk9/yTwW+OL8lOZ+SDw3ILV3bJuBOYz838z8yCtK5uuGUXOhbrk7maScn87M/+jev4/wFO0Plldh2PeLXs3E5E9W5rV4jnVI6nBMR9EnQq92+0FJlUC/xIRe6pbHgBMZ3V9fvX1JWNL11u3rHX4PWyp7vq5o+1P6InMHRGrgV+hdcZYq2O+IDtM+HGPiGUR8ShwFPhiZtbumPdSp0Lv6/YCE+TXM/NqWneifGdEvHbcgYZk0n8PHwNeDryK1r2E7qjWT1zuiJgCPgP8UWb+YLGhHdZNWvaJP+6ZeTIzX0Xrk+zXRMTliwyfmNyDqFOh1+r2Apl5pPp6FPgnWn+ufSciLgaovh4dX8KeumWd6N9DZn6n+of7Y+Bv+OmfyROVOyLOoVWIf5eZ91ara3HMO2Wvy3EHyMzvAw1gPTU55v2qU6H3cwuCiRAR50XE+aeeA28C9tLK+/Zq2NuBz40nYV+6Zd0J3BARL4iIS2ndA/+rY8jX0al/nJXfpnXcYYJyR0QAnwCeysy/ats08ce8W/ZJP+4R8eKIuLB6/kLgN4H/ogbHfCDjfld2kAet2ws8Tesd5/eMO88iOV9G6x3yx4B9p7ICFwFfAr5efX3RuLNWuT5N68/kH9E6M7llsazAe6rfwX5gw4Tl/lvgCeBxWv8oL57A3K+h9ef748Cj1eO6mhzzbtkn+rgDVwL/WeXbC7y3Wj/xx3yQhx/9l6RC1GnKRZK0CAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFeL/AHIC55z3g6RuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.zh_cn.map(lambda x: len(sp.encode_as_pieces(x))).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THnJCmQkTkkL"
   },
   "source": [
    "`MarianMT` also requires `vocab.json` with all vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sRxK8y7KTkkL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#read vocab\n",
    "with open('marian-mt-zh_cn-th/both.vocab','r') as f:\n",
    "    vocab_lines = f.readlines()\n",
    "vocab_lines[:10]\n",
    "vocab_dict = {j.split('\\t')[0]:i for i,j in enumerate(vocab_lines)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t6eXgPE9TkkM"
   },
   "outputs": [],
   "source": [
    "#save vocab\n",
    "with open('marian-mt-zh_cn-th/vocab.json','w') as f:\n",
    "    json.dump(vocab_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfCnTTcmTkkM"
   },
   "source": [
    "Once we have trained the `sentencepiece` tokenizer and all the necessary preparations, we can load the tokenizer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gNodrh4MTkkM"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('marian-mt-zh_cn-th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "b49ac3f5-f134-4063-9281-9258220427dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [18748, 1237, 6525, 508, 2], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AG9c1Z2NTkkO",
    "outputId": "22c60ebe-0e2b-448c-8dd8-2c24c3660b55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\").input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "Instead of one sentence, we can pass along a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSphcukK0RWC",
    "outputId": "86a41356-580e-4907-d1bb-6def8ed89e07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[18748, 5980, 2], [1481, 2064, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á\", \"‰Ω†Â•ΩÂÖÑÂºü\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBgu8aSC0RWD"
   },
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2V0NAmg0RWD",
    "outputId": "c66213f2-a2c9-49ee-cdc8-f6ddfd4284ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[18748, 5980, 2], [1481, 2064, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á\", \"‰Ω†Â•ΩÂÖÑÂºü\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU0EBRlT0RWG"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"zh_cn\"\n",
    "target_lang = \"th\"\n",
    "\n",
    "def preprocess_function(examples, tokenizer, prefix=''):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-b70jh26IrJS",
    "outputId": "a28cd570-0f18-4642-ba44-bd4261002f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2532, 8314, 125, 9278, 131, 1111, 47, 248, 2], [162, 6, 100, 15975, 289, 3838, 221, 12056, 160, 6272, 6, 34, 717, 2281, 1746, 3242, 8, 102, 13892, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[13251, 22, 27090, 25, 1495, 2], [1347, 1237, 642, 6055, 61, 3863, 1800, 9698, 1073, 2597, 483, 36, 5061, 11084, 9, 2082, 4514, 733, 19, 6163, 23525, 230, 2]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "5a84a038e8984a918cf57c1dd341d1e6",
      "b9ddacae92bc42259d05b87dfaf416a3",
      "5d0093b629254596a3689aff7da178e4",
      "dd2c0353f26a49568df2315a20c5d2ed",
      "506cbad6ef8246c9a62b2ca179ad5596",
      "c0bde5db02024727aae4032273c4b073",
      "3b70fe99f487492f9b5e61db6dd6aee9",
      "459630dea34b44ff999d746b2b456f6d",
      "e8cd66a7c0d3415fbf733ed8d8a19ae6",
      "2bf5e74c85184db58bee9e68a35d1118",
      "068b899731154f54ba430ed7b9897cad",
      "ff7a31a51e1b4ebe942fdf38630daf84",
      "cb8a5d06528d4d0098cb69ca14a165e7",
      "80c05f8dd2624632b6d7a73bd9b6056e",
      "a7022017c9a64c068c4ccc2151eda0f3",
      "11595089ef894a8bb6291d34dc6ba1a4"
     ]
    },
    "id": "DDtsaJeVIrJT",
    "outputId": "30fa14b7-0443-4440-8105-718dc6dddb83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6d8e063080441a900d0491fb7bdb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1226.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8716c4c0962b4a5eb683692dec3095c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(partial(preprocess_function, tokenizer=tokenizer), \n",
    "                                      batched=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the ü§ó Datasets library to avoid spending time on this step the next time you run your notebook. The ü§ó Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ü§ó Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Pretraining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw7Gain3TkkX"
   },
   "source": [
    "In order to be able to pretrain the model from scratch we copy the `config.json` from [`Helsinki-NLP/opus-mt-en-ro`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) and modify it as follows to save in `marian-mt-zh_cn-th`. The only parts you might need to change are:\n",
    "```\n",
    "  \"bos_token_id\": 1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"pad_token_id\": 3,\n",
    "  \"unk_token_id\": 0,\n",
    "  \"decoder_start_token_id\": 3,\n",
    "  \"vocab_size\": 32001,\n",
    " ```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "UbFJiwdYTkkY"
   },
   "source": [
    "{\n",
    "  \"_num_labels\": 3,\n",
    "  \"activation_dropout\": 0.0,\n",
    "  \"activation_function\": \"swish\",\n",
    "  \"add_bias_logits\": false,\n",
    "  \"add_final_layer_norm\": false,\n",
    "  \"architectures\": [\n",
    "    \"MarianMTModel\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bad_words_ids\": [\n",
    "    [\n",
    "      3\n",
    "    ]\n",
    "  ],\n",
    "  \"bos_token_id\": 1,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"pad_token_id\": 3,\n",
    "  \"unk_token_id\": 0,\n",
    "  \"decoder_start_token_id\": 3,\n",
    "  \"vocab_size\": 32001,\n",
    "  \"classif_dropout\": 0.0,\n",
    "  \"d_model\": 512,\n",
    "  \"decoder_attention_heads\": 8,\n",
    "  \"decoder_ffn_dim\": 2048,\n",
    "  \"decoder_layerdrop\": 0.0,\n",
    "  \"decoder_layers\": 6,\n",
    "  \"dropout\": 0.1,\n",
    "  \"encoder_attention_heads\": 8,\n",
    "  \"encoder_ffn_dim\": 2048,\n",
    "  \"encoder_layerdrop\": 0.0,\n",
    "  \"encoder_layers\": 6,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\",\n",
    "    \"2\": \"LABEL_2\"\n",
    "  },\n",
    "  \"init_std\": 0.02,\n",
    "  \"is_encoder_decoder\": true,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "    \"LABEL_2\": 2\n",
    "  },\n",
    "  \"max_length\": 512,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"marian\",\n",
    "  \"normalize_before\": false,\n",
    "  \"normalize_embedding\": false,\n",
    "  \"num_beams\": 4,\n",
    "  \"num_hidden_layers\": 6,\n",
    "  \"scale_embedding\": true,\n",
    "  \"static_position_embeddings\": true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MarianMTModel, \n",
    "    MarianConfig, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "config = MarianConfig.from_pretrained('marian-mt-zh_cn-th')\n",
    "model = MarianMTModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"zh_cn-th-translation\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
    "\n",
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "sWsUjLDp0RWL"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"], \n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "uNx5pyRlIrJh",
    "outputId": "8aa9c93d-e9b9-4636-935b-3c0ad257da54",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcstorm125\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">zh_cn-th-translation</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/cstorm125/huggingface\" target=\"_blank\">https://wandb.ai/cstorm125/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/cstorm125/huggingface/runs/1rwpnakk\" target=\"_blank\">https://wandb.ai/cstorm125/huggingface/runs/1rwpnakk</a><br/>\n",
       "                Run data is saved locally in <code>/home/cstorm125/NLP-ZH_TH-Project/notebooks/wandb/run-20210617_144614-1rwpnakk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 15.78 GiB total capacity; 14.24 GiB already allocated; 66.75 MiB free; 14.41 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m         )\n\u001b[1;32m   1283\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m         )\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m                 )\n\u001b[1;32m   1021\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# Cross-Attention Block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 171\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2200\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m         )\n\u001b[0;32m-> 2202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 15.78 GiB total capacity; 14.24 GiB already allocated; 66.75 MiB free; 14.41 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "jQo1UeceijTh"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "trainer.save_model('trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEvEDAsyi2D9",
    "outputId": "2bb85250-02b7-4cff-c5f0-218813e17700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '\"']"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer sample sentence zh_cn to th\n",
    "model.cpu()\n",
    "src_text = [\n",
    "    'ÊàëÁà±‰Ω†',\n",
    "    'ÂõΩÁéãÊúâÂæàÂ§öÂøÉ‰∫ã„ÄÇÊàëÊòéÁôΩ'\n",
    "]\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fprJxAS20RWN"
   },
   "source": [
    "Don't forget to [upload your model](https://huggingface.co/transformers/model_sharing.html) on the [ü§ó Model Hub](https://huggingface.co/models). You can then use it only to generate results like the one shown in the first picture of this notebook!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "huggingface_zhth_translation_tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "068b899731154f54ba430ed7b9897cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80c05f8dd2624632b6d7a73bd9b6056e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb8a5d06528d4d0098cb69ca14a165e7",
      "value": 1
     }
    },
    "11595089ef894a8bb6291d34dc6ba1a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bf5e74c85184db58bee9e68a35d1118": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b70fe99f487492f9b5e61db6dd6aee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "459630dea34b44ff999d746b2b456f6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506cbad6ef8246c9a62b2ca179ad5596": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5a84a038e8984a918cf57c1dd341d1e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d0093b629254596a3689aff7da178e4",
       "IPY_MODEL_dd2c0353f26a49568df2315a20c5d2ed"
      ],
      "layout": "IPY_MODEL_b9ddacae92bc42259d05b87dfaf416a3"
     }
    },
    "5d0093b629254596a3689aff7da178e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0bde5db02024727aae4032273c4b073",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_506cbad6ef8246c9a62b2ca179ad5596",
      "value": 9
     }
    },
    "80c05f8dd2624632b6d7a73bd9b6056e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7022017c9a64c068c4ccc2151eda0f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9ddacae92bc42259d05b87dfaf416a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0bde5db02024727aae4032273c4b073": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb8a5d06528d4d0098cb69ca14a165e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dd2c0353f26a49568df2315a20c5d2ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_459630dea34b44ff999d746b2b456f6d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3b70fe99f487492f9b5e61db6dd6aee9",
      "value": " 9/9 [01:36&lt;00:00, 10.70s/ba]"
     }
    },
    "e8cd66a7c0d3415fbf733ed8d8a19ae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_068b899731154f54ba430ed7b9897cad",
       "IPY_MODEL_ff7a31a51e1b4ebe942fdf38630daf84"
      ],
      "layout": "IPY_MODEL_2bf5e74c85184db58bee9e68a35d1118"
     }
    },
    "ff7a31a51e1b4ebe942fdf38630daf84": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11595089ef894a8bb6291d34dc6ba1a4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a7022017c9a64c068c4ccc2151eda0f3",
      "value": " 1/1 [00:14&lt;00:00, 14.42s/ba]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
